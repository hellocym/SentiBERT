{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Input,Flatten,Embedding, Dropout,Conv1D,MaxPooling1D, Dense, GlobalMaxPooling1D,BatchNormalization, Add,GlobalAveragePooling1D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications import VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data/training.1600000.processed.noemoticon.csv',encoding = 'latin',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = data[[5, 0]]\n",
    "data.columns=['tweet', 'sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data['sentiment'] = data['sentiment'].replace(4,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "def preprocess(text):\n",
    "    # Removing URLS\n",
    "    text = re.sub(r\"https?://\\S+|www\\.\\S+\",\" \",text)\n",
    "    \n",
    "    # Removing html tags\n",
    "    text = re.sub(r\"<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});\",\" \",text)\n",
    "    \n",
    "    # Removing the Punctuation\n",
    "    text = re.sub(r\"[^\\w\\s]\", \" \", text)\n",
    "        \n",
    "    # Removing words that have numbers \n",
    "    text = re.sub(r\"\\w*\\d\\w*\", \" \", text)\n",
    "        \n",
    "    # Removing Digits \n",
    "    text = re.sub(r\"[0-9]+\", \" \", text)\n",
    "        \n",
    "    # Cleaning white spaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "        \n",
    "    text = text.lower()\n",
    "    # Check stop words\n",
    "    tokens = []\n",
    "    for token in text.split():\n",
    "        if token not in stop_words and len(token) > 3:\n",
    "            tokens.append(token)\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data.loc[:, 'tweet'] = data['tweet'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data size: 1120000 1120000\n",
      "Validation Data size: 320000 320000\n",
      "Test Data size 160000 160000\n"
     ]
    }
   ],
   "source": [
    "X = data['tweet']\n",
    "y = data['sentiment']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=7)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=2./9, random_state=7)\n",
    "\n",
    "print(\"Train Data size:\", len(X_train), len(y_train))\n",
    "print(\"Validation Data size:\", len(X_val), len(y_val))\n",
    "print(\"Test Data size\", len(X_test), len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# Convert text to sequences of integers\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_val = tokenizer.texts_to_sequences(X_val)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After padding: (1120000, 27)\n",
      "After padding: (320000, 27)\n",
      "After padding:(160000, 27)\n"
     ]
    }
   ],
   "source": [
    "max_length = max([len(seq) for seq in X_train])\n",
    "X_train = pad_sequences(X_train, maxlen=max_length)\n",
    "X_val = pad_sequences(X_val, maxlen=max_length)\n",
    "X_test = pad_sequences(X_test, maxlen=max_length)\n",
    "print(f\"After padding: {X_train.shape}\")\n",
    "print(f\"After padding: {X_val.shape}\")\n",
    "print(f\"After padding:{X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.save('X_train.npy', X_train)\n",
    "np.save('X_val.npy', X_val)\n",
    "np.save('X_test.npy', X_test)\n",
    "np.save('y_train.npy', y_train)\n",
    "np.save('y_val.npy', y_val)\n",
    "np.save('y_test.npy', y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
